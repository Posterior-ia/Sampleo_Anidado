{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5e5693-46e8-4cd5-8d5c-2fccd9acc109",
   "metadata": {},
   "source": [
    "# Jugando con nested sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c15fdf-883f-424f-9f54-ba4709d46f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "rng = np.random.default_rng(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a0064-e0d4-434b-83ca-e2205cc4c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(x, y, a, b):\n",
    "    \"\"\"\n",
    "    Typical concave function used to test optimization\n",
    "    algorithms\n",
    "    \"\"\"\n",
    "    return (a - x) ** 2 + b * (y - x ** 2) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a773446-df77-4890-b742-022a793c1be3",
   "metadata": {},
   "source": [
    "Para empezar, vamos a usar una verosimilitud de Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c995c885-c520-46f7-84bf-7b7ac96596a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood(x, y):\n",
    "    return 1 / (1 +  rosenbrock(x, y, a=0.5, b=100))\n",
    "\n",
    "xcoord1d = np.linspace(-2, 2, 250)\n",
    "ycoord1d = np.linspace(-1, 3, 250)\n",
    "x, y = np.meshgrid(xcoord1d, ycoord1d)\n",
    "a = 0.5\n",
    "b = 100\n",
    "L = likelihood(x, y)\n",
    "dx_grid = xcoord1d[1] - xcoord1d[0]\n",
    "dy_grid = ycoord1d[1] - ycoord1d[0]\n",
    "\n",
    "true_evidence = np.sum(L) * dx_grid * dy_grid\n",
    "print(f'True evidence = {true_evidence:.2}')\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "extent = (xcoord1d[0], xcoord1d[-1], ycoord1d[0], ycoord1d[-1])\n",
    "im_ = ax.imshow(\n",
    "    L,\n",
    "    origin='lower',\n",
    "    norm=LogNorm(),\n",
    "    extent=extent,\n",
    ")\n",
    "ax.contour(\n",
    "    L,\n",
    "    levels=np.array([0.2, 0.5, 0.9]) * L.max(), \n",
    "    extent=extent,\n",
    "    colors='white'\n",
    ")\n",
    "\n",
    "fig.colorbar(im_, shrink=0.82)\n",
    "\n",
    "ax.axvline(a, color='red')\n",
    "ax.axhline(a ** 2, color='red')\n",
    "ax.plot([a], [a ** 2], marker='s', ls='', color='red', mec='k')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f1a83-99a6-4651-96a7-47b971f1c7c2",
   "metadata": {},
   "source": [
    "## Inicialización\n",
    "El primer paso es hacer un muestreo de los priors con 100 puntos vivos y evaluar la verosimilitud $L$ en cada punto. En este caso voy a usar priors uniformes entre $-2<x<2$ y $-1<y<3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cca3614-e9ed-4e23-beba-c0463233da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Nlive = 100\n",
    "x_live = rng.uniform(low=-2, high=2, size=Nlive)\n",
    "y_live = rng.uniform(low=-1, high=3, size=Nlive)\n",
    "\n",
    "live_likelihoods = likelihood(x_live, y_live)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(L, extent=extent, origin='lower', norm=LogNorm())\n",
    "ax.scatter(x_live, y_live, marker='o', color='red');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93693fe-948c-4384-9874-6d78db5ebc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.sort(live_likelihoods), ls='', marker='o')\n",
    "ax.set_yscale('log')\n",
    "ax.set_ylabel('Likelihood')\n",
    "ax.set_xlabel('Sorted live point index')\n",
    "ax.annotate('Lowest likelihood point', (0, live_likelihoods.min()*1.1), xytext=(0, 1e-2), arrowprops=dict(width=1, color='k'));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24c89c-1bdf-49c3-afa9-e02bc56bf9ec",
   "metadata": {},
   "source": [
    "## Reducción\n",
    "Ahora tengo que eliminar el punto con menor verosimilitud $L_0$. Cada punto vivo representa aproximadapente $1/N$ del volumen total. Por lo tanto, eliminar este punto reduce el volumen por un diferencial de aproximadamente $\\delta V \\approx 1/N$. Para ser más precisos, $\\delta V$ es una variable aleatoria que se distribuye según $\\mathrm{Beta}(1, N)$. Entonces, para estimar $\\delta V$ uno puede tomar una muestra de dicha distribución, o bien, usar la media geométrica $1 - \\exp(-1/N)$ o la media aritmética $1/(N+1)$. En la práctica, da un poco lo mismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870ab4d1-f7b7-4fee-a66f-c53e18a025c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_L0 = np.argmin(live_likelihoods)\n",
    "dead_points = list()\n",
    "dead_points.append((x_live[index_L0], y_live[index_L0],  live_likelihoods[index_L0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6195e0d3-5f8b-44aa-9386-ade8809bc87a",
   "metadata": {},
   "source": [
    "## Muestreo de prior con verosimilitud restringida\n",
    "Una vez que eliminamos nuestro punto de más baja verosimilitud, tenemos que reemplazarlo por un punto cuya verosimilitud sea mayor. En inglés esto se llama _likelihood-restricted prior sampling_ o LPRS y es el paso más importante de NS. Sin embargo, hacer esto de manera eficiente no es tan sencillo. La estrategia de fuerza bruta es volver a muestrear aleatoriamente el prior varias veces hasta que salga un punto con $L>L_0$. Para hacer esto un poquito más eficiente, podemos restringir el espacio prior al rectángulo más pequeño que contenga todos los puntos vivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd9ca6c-8135-4b98-8f65-cd64702de006",
   "metadata": {},
   "outputs": [],
   "source": [
    "L_new = 0\n",
    "while L_new <= live_likelihoods[index_L0]:\n",
    "    x_new = rng.uniform(low=x_live.min(), high=x_live.max())\n",
    "    y_new = rng.uniform(low=y_live.min(), high=y_live.max())\n",
    "    L_new = likelihood(x_new, y_new)\n",
    "\n",
    "x_live[index_L0] = x_new\n",
    "y_live[index_L0] = y_new\n",
    "live_likelihoods[index_L0] = L_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372698aa-8d7c-4fd9-91bb-f1858abd128f",
   "metadata": {},
   "source": [
    "## Iteración\n",
    "Veamos hasta dónde llegamos despúes de 1000 iteraciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d979fa63-2f88-4222-a0b6-e27e5594b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(1_000):\n",
    "    index_L0 = np.argmin(live_likelihoods)\n",
    "    dead_points.append((x_live[index_L0], y_live[index_L0],  live_likelihoods[index_L0]))\n",
    "    L_new = 0\n",
    "    while L_new < live_likelihoods[index_L0]:\n",
    "        x_new = rng.uniform(low=x_live.min(), high=x_live.max())\n",
    "        y_new = rng.uniform(low=y_live.min(), high=y_live.max())\n",
    "        L_new = inverse_rosenbrock(x_new, y_new, a, b)\n",
    "\n",
    "    x_live[index_L0] = x_new\n",
    "    y_live[index_L0] = y_new\n",
    "    live_likelihoods[index_L0] = L_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affd66eb-6600-44c3-b7f6-27475dc4f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dead_points_array = np.array(dead_points).T\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(dead_points_array[2])\n",
    "ax.set_xlabel('Número de iteración')\n",
    "ax.set_ylabel(r'$L_0$')\n",
    "ax.set_yscale('log');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69bcdc1a-9a6c-4587-a849-b30ae2b21927",
   "metadata": {},
   "source": [
    "## Integración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca70038-2780-4ccd-854a-4b280da75bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "volume_array = (1 - 1 / Nlive)  ** np.arange(dead_points_array.shape[1]) * (xcoord1d[-1] - xcoord1d[0]) * (ycoord1d[-1] - ycoord1d[0])\n",
    "dV_array = volume_array * 1 / Nlive\n",
    "weights = dead_points_array[2] * dV_array\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(-np.log(volume_array), weights*1e4)\n",
    "ax.set_xlabel(r'-log(Volume)')\n",
    "ax.set_ylabel(r'Weights $(\\Delta V_i \\times L_i$)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cdc25f-6661-40f2-aa8e-d09749a47db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(-np.log(volume_array), np.cumsum(weights))\n",
    "ax.set_xlabel('-log(Volume)')\n",
    "ax.set_ylabel('Evidence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3758549-45a2-4ea2-ad06-c8771bbf0613",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dead_points_array[0], dead_points_array[1], c=np.cumsum(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78be7841-3cd4-4929-adeb-84f66fd40166",
   "metadata": {},
   "source": [
    "Sugerencias Nico para avanzar:\n",
    " - Usar github (coordinar con Gabriel)\n",
    " - Comparar con integración por grilla\n",
    " - Entender de dónde sale la distribución Beta\n",
    " - Separar los artículos en una serie (ns básico, algoritmos de LRPS, estado del arte, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ac727c-da21-4a69-95a9-ad43659c1d3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
